{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code.model.main import End_to_end_neural_el, preprocessing, prepro_util_main\n",
    "\n",
    "print (\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object creation\n",
    "\n",
    "end_to_end_el_obj = End_to_end_neural_el()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing data\n",
    "input_file_path = \"../data/basic_data/test_datasets/AIDA/\"\n",
    "output_file_path = \"../data/new_datasets/\"\n",
    "dataset_name = \"AIDA\"\n",
    "preprocessing(dataset_name, input_folder, output_folder)\n",
    "prepro_util_main()\n",
    "\n",
    "print (\"preprocessing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessed data\n",
    "# Prepare Arguments for training - These were  read from command line in the original implementation\n",
    "args_dict = {}\n",
    "args_dict['batch_size'] = 4\n",
    "args_dict['experiment_name'] = \"paper_models\"\n",
    "args_dict['training_name'] = \"group_global / global_model_v$v\"\n",
    "args_dict['ent_vecs_regularization'] = \"l2dropout\"\n",
    "args_dict['evaluation_minutes'] = 10\n",
    "args_dict['nepoch_no_imprv'] = 6\n",
    "args_dict['span_emb'] = \"boundaries\"\n",
    "args_dict['dim_char'] = 50\n",
    "args_dict['hidden_size_char'] = 50\n",
    "args_dict['hidden_size_lstm'] = 150\n",
    "args_dict['nn_components'] = \"pem_lstm_attention_global\"\n",
    "args_dict['fast_evaluation'] = True\n",
    "args_dict['all_spans_training'] = True\n",
    "args_dict['attention_ent_vecs_no_regularization'] = True\n",
    "args_dict['final_score_ffnn'] = \"0_0\"\n",
    "args_dict['attention_R'] = 10\n",
    "args_dict['attention_K'] = 100\n",
    "args_dict['train_datasets'] = \"aida_train\"\n",
    "args_dict['el_datasets'] = \"aida_dev_z_aida_test_z_aida_train\"\n",
    "args_dict['el_val_datasets'] = 0\n",
    "args_dict['global_thr'] = 0.001\n",
    "args_dict['global_score_ffnn'] = \"0_0\"\n",
    "\n",
    "args = _parse_args_train(args_dict)\n",
    "\n",
    "# Read data for EL Task\n",
    "\n",
    "folder = \"../data/tfrecords/\"+args_dict['experiment_name']+\\\n",
    "                (\"/allspans/\" if args.all_spans_training else \"/gmonly/\")\n",
    "dataset_path = [folder + file for file in args.train_datasets]\n",
    "training_dataset = end_to_end_el_obj.read_dataset(dataset_path, args_dict)\n",
    "\n",
    "print (\"Reading for EL done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for EL task\n",
    "trained_model_ed = end_to_end_el_obj.train([training_dataset])\n",
    "\n",
    "print (\"Training for EL done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for ED Task\n",
    "args_dict['all_spans_training'] = False\n",
    "args = _parse_args_train(args_dict)\n",
    "\n",
    "folder = \"../data/tfrecords/\" + args_dict['experiment_name'] + \\\n",
    "             (\"/allspans/\" if args.all_spans_training else \"/gmonly/\")\n",
    "dataset_path = [folder + file for file in args.train_datasets]\n",
    "training_dataset = end_to_end_el_obj.read_dataset(dataset_path, args_dict)\n",
    "\n",
    "print (\"Reading for ED done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for ED task\n",
    "trained_model_ed = end_to_end_el_obj.train([training_dataset])\n",
    "\n",
    "print (\"Training for ED done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "prediction_datasets = [\"aida_train.txt_z_aida_dev.txt_z_aida_test.txt\"]  # <-- as we need list input\n",
    "predictions = end_to_end_el_obj.predict(trained_model_el, prediction_datasets)\n",
    "\n",
    "print (\"Predicting done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluations\n",
    "base_folder = \"../../data/tfrecords/\"\n",
    "dev_set = \"aida_dev.txt\"\n",
    "test_set = \"aida_test.txt\"\n",
    "required_data = [base_folder, dev_set, test_set]\n",
    "precision, recall, F1 = end_to_end_el_obj.evaluate(trained_model_el, required_data)\n",
    "\n",
    "print (\"Accuracy :\", precision, recall, F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
