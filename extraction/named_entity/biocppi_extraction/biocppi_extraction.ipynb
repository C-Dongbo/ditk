{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from biocppi_utils import copy_predictions_to_predictions_with_header, load_groundTruth_from_predictions\n",
    "from biocppi_extraction import biocppi_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model with custom parameters\n",
    "### Note that these parameters are tuned to allow runtime to be low, for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = '/home/kcrouch/USC/csci_548/project/groupedProject/bioNER_refactor/biocppi_extraction/embeddings/PubMed-w2v.txt'\n",
    "test_params = {'num_ensembles':2,'num_iterations':1000,'num_it_per_ckpt':100}  # note, if num_it_per_ckpt > num_iterations then num_it_per_ckpt will be set to half of num_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the file information for the dataset you want to train and test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate on small version of conll2003\n",
    "dataset_name = 'CoNLL_2003'\n",
    "dataset_dir = '/home/kcrouch/smol_datasets/conll/'  # smol sample\n",
    "raw_data_train_file = dataset_dir + 'train.txt'\n",
    "raw_data_dev_file = dataset_dir + 'dev.txt'\n",
    "raw_data_test_file = dataset_dir + 'test.txt'\n",
    "file_dict = {'train':{'data':raw_data_train_file},'dev':{'data':raw_data_dev_file},'test':{'data':raw_data_test_file}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biocppi = biocppi_extraction(embeddings_path=embeddings_path,**test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = biocppi.read_dataset(file_dict, dataset_name)  # data read, converted, and written to files in proper location expected by train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using the training data we just read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BiLSTM model\n",
      "Will train 2 total models [num_ensembles]\n",
      "train.txt\n",
      "Loaded 170 instances from data set\n",
      "Saved vocab to corpus_train/word_vocab.ner.txt\n",
      "Loading embeddings.. Organizing embeddings.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset..\n",
      "Loaded 170 instances with a vocab size of 1081 from train.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done f\n",
      "74s\n",
      "Vectorizing embeddings 33/1081   \r",
      "Vectorizing embeddings 66/1081   \r",
      "Vectorizing embeddings 99/1081   \r",
      "Vectorizing embeddings 132/1081   \r",
      "Vectorizing embeddings 165/1081   \r",
      "Vectorizing embeddings 198/1081   \r",
      "Vectorizing embeddings 231/1081   \r",
      "Vectorizing embeddings 264/1081   \r",
      "Vectorizing embeddings 297/1081   \r",
      "Vectorizing embeddings 330/1081   \r",
      "Vectorizing embeddings 363/1081   \r",
      "Vectorizing embeddings 396/1081   \r",
      "Vectorizing embeddings 429/1081   \r",
      "Vectorizing embeddings 462/1081   \r",
      "Vectorizing embeddings 495/1081   \r",
      "Vectorizing embeddings 528/1081   \r",
      "Vectorizing embeddings 561/1081   \r",
      "Vectorizing embeddings 594/1081   \r",
      "Vectorizing embeddings 627/1081   \r",
      "Vectorizing embeddings 660/1081   \r",
      "Vectorizing embeddings 693/1081   \r",
      "Vectorizing embeddings 726/1081   \r",
      "Vectorizing embeddings 759/1081   \r",
      "Vectorizing embeddings 792/1081   \r",
      "Vectorizing embeddings 825/1081   \r",
      "Vectorizing embeddings 858/1081   \r",
      "Vectorizing embeddings 891/1081   \r",
      "Vectorizing embeddings 924/1081   \r",
      "Loaded 954/1081 embedding vectors\n",
      "Loaded weight matrix (1081, 200)..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kcrouch/USC/csci_548/project/groupedProject/bioNERlocal/env_setups/env__biocppi_extraction/biocppi/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 68, tuning on 17\n",
      "Target labels: ['B-MISC', 'I-MISC', 'O']\n",
      "68/17 in training/validation set\n",
      "Using batch_size of 16\n",
      "ckpt 0 bsize=16 loss 0.00537135824561 fit 100.00% val 94.49%/88.89%/96.00%[1m10s] *\n",
      "ckpt 1 bsize=16 loss 0.000595263554715 fit 100.00% val 93.39%/84.21%/96.00%[1m6s] \n",
      "ckpt 2 bsize=16 loss 0.000265994487563 fit 100.00% val 94.12%/87.27%/96.00%[1m8s] \n",
      "ckpt 3 bsize=16 loss 0.000150912877871 fit 100.00% val 94.12%/87.27%/96.00%[1m8s] \n",
      "ckpt 4 bsize=16 loss 0.000100554221717 fit 100.00% val 95.70%/87.50%/98.00%[1m5s] *\n",
      "ckpt 5 bsize=16 loss 7.19405870768e-05 fit 100.00% val 95.70%/87.50%/98.00%[1m5s] *\n",
      "ckpt 6 bsize=16 loss 5.52921883354e-05 fit 100.00% val 95.70%/87.50%/98.00%[1m6s] *\n",
      "ckpt 7 bsize=16 loss 4.50886582257e-05 fit 100.00% val 95.70%/87.50%/98.00%[1m37s] *\n",
      "ckpt 8 bsize=16 loss 3.65110463463e-05 fit 100.00% val 95.70%/87.50%/98.00%[1m18s] *\n",
      "ckpt 9 bsize=16 loss 3.06697911583e-05 fit 100.00% val 95.70%/87.50%/98.00%[1m4s] *\n",
      "Fitted to model from chkpt 9 with score 0.95703125 at scratch/model-1556776962-bilstm-e9-s0.ckpt\n",
      "Saved model 0 to corpus_train//saved_model_autumn//model_0\n",
      "Training on 68, tuning on 17\n",
      "Target labels: ['B-MISC', 'I-MISC', 'O']\n",
      "68/17 in training/validation set\n",
      "Using batch_size of 16\n",
      "ckpt 0 bsize=16 loss 0.00449515366927 fit 100.00% val 94.80%/98.41%/93.94%[1m12s] *\n",
      "ckpt 1 bsize=16 loss 0.000525187060703 fit 100.00% val 94.80%/98.41%/93.94%[1m8s] *\n",
      "ckpt 2 bsize=16 loss 0.000221005437197 fit 100.00% val 94.80%/98.41%/93.94%[1m14s] *\n",
      "ckpt 3 bsize=16 loss 0.000127707840875 fit 100.00% val 94.80%/98.41%/93.94%[1m11s] *\n",
      "ckpt 4 bsize=16 loss 8.71164011187e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m13s] *\n",
      "ckpt 5 bsize=16 loss 6.35677570244e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m14s] *\n",
      "ckpt 6 bsize=16 loss 4.75766391901e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m19s] *\n",
      "ckpt 7 bsize=16 loss 3.77708092856e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m15s] *\n",
      "ckpt 8 bsize=16 loss 3.12235424644e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m15s] *\n",
      "ckpt 9 bsize=16 loss 2.60856431851e-05 fit 100.00% val 94.80%/98.41%/93.94%[1m17s] *\n",
      "Fitted to model from chkpt 9 with score 0.948012232416 at scratch/model-1556777676-bilstm-e9-s1.ckpt\n",
      "Saved model 1 to corpus_train//saved_model_autumn//model_1\n",
      "DONE TRAIN\n"
     ]
    }
   ],
   "source": [
    "data_train = data['train']  # test passing actual data [empty also works]\n",
    "biocppi.train(data_train)\n",
    "print('DONE TRAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, make predictions on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE PREDICT\n"
     ]
    }
   ],
   "source": [
    "data_test = data['test']\n",
    "predictions = biocppi.predict(data_test)  # test passing actual data [empty also works]\n",
    "print('DONE PREDICT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict() generates a `predictions.txt` file. This file is loaded into evaluate() to calculate our precision, recall, and F1 scores! The results here will not be very accurate, but try training on a full dataset, with a larger number of ensembles and a higher number of iterations ;)\n",
    "## So, let's evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp 16.0\n",
      "fp 2.0\n",
      "fn 48.0\n",
      "final evaluation scores: precision=0.888888888889, recall=0.25, f1=0.390243902439\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = biocppi.evaluate(None,None)  # passing None for predictions and groundTruth parameters allows evaluate() to just read in the information from predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8888888888888888, 0.25, 0.3902439024390244)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show evaluation results tuple\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocppi",
   "language": "python",
   "name": "biocppi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
