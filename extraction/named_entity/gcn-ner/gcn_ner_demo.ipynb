{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook for demonstrating NER using Graph Convolutional Networks\n",
    "\n",
    "### 1. Import the GCNNer class from ner_gcn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_gcn import GcnNer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an object of the class. Initialize a dictionary with the required data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/ner-gcn-21.tf\n"
     ]
    }
   ],
   "source": [
    "model = GcnNer()\n",
    "fileDict = {'train': './data/train.conll', 'dev': './data/dev.conll', 'test': './data/test.conll'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Call read_dataset to read the data from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_data, model.test_data = model.read_dataset(file_dict = fileDict, dataset_name = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Computing the transition matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------- Epoch 0 ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------- Epoch 1 ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------- Epoch 2 ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n",
      "invalid continue\n",
      "Model saved at  ./data/ner-gcn-2.tf\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "model.train(data = model.train_data, epochs = 3)\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate the ground truth tuples for evaluation.\n",
    "Format: (word, true_entity_type, start_position, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_tuples = model.convert_ground_truth(model.predict_data, fileDict['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Call the predict method with the input data.\n",
    "Format of input data is the DITK format explained in the readme file.\n",
    "You can pass the pretrained model if load_model is called instead of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/ner-gcn-21.tf\n",
      "*************Predicted entity types: [word, predicted_type, start_position, span]**************\n",
      "('Yes', 'O', 0, 3)\n",
      "('they', 'O', 4, 4)\n",
      "('did', 'O', 9, 3)\n",
      "('/.', 'O', 13, 2)\n",
      "('and', 'O', 16, 3)\n",
      "('they', 'O', 20, 4)\n",
      "('were', 'O', 25, 4)\n",
      "('not', 'O', 30, 3)\n",
      "('the', 'O', 34, 3)\n",
      "('first', 'B-ORDINAL', 38, 5)\n",
      "('company', 'O', 44, 7)\n",
      "('that', 'O', 52, 4)\n",
      "('approached', 'O', 57, 10)\n",
      "('me', 'O', 68, 2)\n",
      "('/.', 'O', 71, 2)\n",
      "('but', 'O', 74, 3)\n",
      "('I', 'O', 78, 1)\n",
      "('am', 'O', 80, 2)\n",
      "('not', 'O', 83, 3)\n",
      "('selling', 'O', 87, 7)\n",
      "('medicine', 'O', 95, 8)\n",
      "('or', 'O', 104, 2)\n",
      "('pharmaceuticals', 'O', 107, 15)\n",
      "('/.', 'O', 123, 2)\n",
      "('I', 'O', 126, 1)\n",
      "(\"'m\", 'O', 128, 2)\n",
      "('sort', 'O', 131, 4)\n",
      "('of', 'O', 136, 2)\n",
      "('about', 'O', 139, 5)\n",
      "('selling', 'O', 145, 7)\n",
      "('a', 'O', 153, 1)\n",
      "('full', 'O', 155, 4)\n",
      "('body', 'O', 160, 4)\n",
      "('approach', 'O', 165, 8)\n",
      "('to', 'O', 174, 2)\n",
      "('wellness', 'O', 177, 8)\n",
      "('/.', 'O', 186, 2)\n",
      "('Um', 'O', 189, 2)\n",
      "('there', 'O', 192, 5)\n",
      "(\"'s\", 'O', 198, 2)\n",
      "('a', 'O', 201, 1)\n",
      "('big', 'O', 203, 3)\n",
      "('gap', 'O', 207, 3)\n",
      "('between', 'O', 211, 7)\n",
      "('those', 'O', 219, 5)\n",
      "('that', 'O', 225, 4)\n",
      "('are', 'O', 230, 3)\n",
      "('mentally', 'O', 234, 8)\n",
      "('ill', 'O', 243, 3)\n",
      "('and', 'O', 247, 3)\n",
      "('the', 'O', 251, 3)\n",
      "('general', 'O', 255, 7)\n",
      "('population', 'O', 263, 10)\n",
      "('/.', 'O', 274, 2)\n",
      "('A', 'O', 277, 1)\n",
      "('much', 'O', 279, 4)\n",
      "('better', 'O', 284, 6)\n",
      "('looking', 'O', 291, 7)\n",
      "('News', 'B-WORK_OF_ART', 299, 4)\n",
      "('Night', 'I-WORK_OF_ART', 304, 5)\n",
      "('I', 'O', 310, 1)\n",
      "('might', 'O', 312, 5)\n",
      "('add', 'O', 318, 3)\n",
      "('as', 'O', 322, 2)\n",
      "('Paula', 'B-PERSON', 325, 5)\n",
      "('Zahn', 'I-PERSON', 331, 4)\n",
      "('sits', 'O', 336, 4)\n",
      "('in', 'O', 341, 2)\n",
      "('for', 'O', 344, 3)\n",
      "('Anderson', 'B-PERSON', 348, 8)\n",
      "('and', 'O', 357, 3)\n",
      "('Aaron', 'B-PERSON', 361, 5)\n",
      "('/.', 'O', 367, 2)\n",
      "('They', 'O', 370, 4)\n",
      "(\"'re\", 'O', 375, 3)\n",
      "('both', 'O', 379, 4)\n",
      "('off', 'O', 384, 3)\n",
      "('/-', 'O', 388, 2)\n",
      "('Look', 'O', 391, 4)\n",
      "('at', 'O', 396, 2)\n",
      "('that', 'O', 399, 4)\n",
      "('/.', 'O', 404, 2)\n"
     ]
    }
   ],
   "source": [
    "entity_tuples = model.predict('./data/input.txt', pretrained_model = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Output of predict is stored in a file.\n",
    "Format is: (WORD TRUE_LABEL PRED_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of predict stored at ./gcn_ner_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "output = open(\"gcn_ner_predictions.txt\", \"w\")\n",
    "output.write(\"WORD TRUE_LABEL PRED_LABEL\"+\"\\n\\n\")\n",
    "for each in entity_tuples:\n",
    "    str = \"\"\n",
    "    str = each[0]+\" \"+each[1]+\" \"+each[2]\n",
    "    output.write(str+\"\\n\")\n",
    "    if (each[0] == \"/.\" or each[0] == \"/-\") and entity_tuples.index(each) != len(entity_tuples)-1:\n",
    "        output.write(\"\\n\")\n",
    "output.close()\n",
    "print(\"Output of predict stored at ./gcn_ner_predictions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Call the evaluate method to obtain the metrics for the predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/ner-gcn-21.tf\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, f1) = model.evaluate(predictions = entity_tuples, groundTruths = ground_truth_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************Evaluation Metrics*************\n",
      "Precision:  100.0\n",
      "Recall:  100.0\n",
      "F1 score:  100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"*************Evaluation Metrics*************\")\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \",recall*100)\n",
    "print(\"F1 score: \", f1*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
